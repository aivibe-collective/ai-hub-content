#!/usr/bin/env python3
"""
A/B Testing Framework for AI Hub Content Creation System.

This script allows for systematic comparison of content generated by different models.
It generates content for the same content items using different models and compares the results.
"""

import os
import sys
import json
import logging
import argparse
import time
from typing import Dict, List, Optional, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Import our custom modules
from supabase_client import (
    is_connected, get_content_inventory, update_content_status,
    get_content_item, update_content_item
)
from content_workflow_supabase import generate_content_for_id
from content_evaluation import ContentEvaluation

# Configure logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ABTest:
    """A/B testing class for content generation."""
    
    def __init__(self, content_ids: List[str], models: List[str], 
                temperatures: Optional[List[float]] = None,
                output_dir: str = 'ab_test_results'):
        """Initialize A/B test.
        
        Args:
            content_ids: List of content IDs to test
            models: List of models to test
            temperatures: List of temperatures to test (optional)
            output_dir: Directory to save results
        """
        self.content_ids = content_ids
        self.models = models
        self.temperatures = temperatures or [0.7]  # Default temperature
        self.output_dir = output_dir
        self.results = []
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    def run_test(self, force: bool = False, delay: int = 0) -> List[Dict]:
        """Run A/B test.
        
        Args:
            force: Whether to force generation even if dependencies aren't met
            delay: Delay between generations in seconds
            
        Returns:
            List of test results
        """
        # Check Supabase connection
        if not is_connected():
            logger.error("Not connected to Supabase")
            return []
        
        # Get content items
        content_items = []
        for content_id in self.content_ids:
            item = get_content_item(content_id)
            if item:
                content_items.append(item)
            else:
                logger.warning(f"Content item {content_id} not found")
        
        if not content_items:
            logger.error("No content items found")
            return []
        
        # Run test for each content item, model, and temperature
        for item in content_items:
            content_id = item['content_id']
            title = item.get('title', content_id)
            
            logger.info(f"Testing content item: {content_id} - {title}")
            
            for model in self.models:
                for temperature in self.temperatures:
                    # Create variant ID
                    variant_id = f"{content_id}_{model.replace('-', '_')}_{temperature}"
                    
                    logger.info(f"Generating variant: {variant_id}")
                    
                    # Save original status
                    original_status = item.get('status')
                    original_model = item.get('model')
                    
                    try:
                        # Set model and temperature
                        update_content_item(content_id, {
                            'model': model,
                            'temperature': temperature
                        })
                        
                        # Reset status to force regeneration
                        update_content_status(content_id, "Not Started")
                        
                        # Generate content
                        start_time = time.time()
                        result = generate_content_for_id(content_id, model, temperature, force=force)
                        generation_time = time.time() - start_time
                        
                        # Check result
                        if result:
                            logger.info(f"Successfully generated variant {variant_id}")
                            
                            # Get content file path
                            content_path = os.path.join('generated_content', f"{content_id}.md")
                            
                            # Copy content to variant file
                            variant_path = os.path.join(self.output_dir, f"{variant_id}.md")
                            with open(content_path, 'r') as src, open(variant_path, 'w') as dst:
                                dst.write(src.read())
                            
                            # Evaluate content
                            evaluation = ContentEvaluation(content_path, content_id)
                            eval_result = evaluation.evaluate()
                            
                            # Save evaluation
                            eval_path = os.path.join(self.output_dir, f"{variant_id}.evaluation.json")
                            with open(eval_path, 'w') as f:
                                json.dump(eval_result, f, indent=2)
                            
                            # Add to results
                            self.results.append({
                                'content_id': content_id,
                                'title': title,
                                'model': model,
                                'temperature': temperature,
                                'variant_id': variant_id,
                                'success': True,
                                'generation_time': generation_time,
                                'scores': eval_result['scores'],
                                'quality_rating': eval_result['quality_rating'],
                                'source_count': eval_result['source_count'],
                                'word_count': eval_result['word_count']
                            })
                        else:
                            logger.error(f"Failed to generate variant {variant_id}")
                            
                            # Add to results
                            self.results.append({
                                'content_id': content_id,
                                'title': title,
                                'model': model,
                                'temperature': temperature,
                                'variant_id': variant_id,
                                'success': False,
                                'generation_time': generation_time
                            })
                    
                    except Exception as e:
                        logger.error(f"Error generating variant {variant_id}: {str(e)}")
                        
                        # Add to results
                        self.results.append({
                            'content_id': content_id,
                            'title': title,
                            'model': model,
                            'temperature': temperature,
                            'variant_id': variant_id,
                            'success': False,
                            'error': str(e)
                        })
                    
                    finally:
                        # Restore original status and model
                        update_content_item(content_id, {
                            'model': original_model
                        })
                        update_content_status(content_id, original_status)
                    
                    # Add delay if specified
                    if delay > 0:
                        logger.info(f"Waiting {delay} seconds before next generation...")
                        time.sleep(delay)
        
        # Save results
        self.save_results()
        
        return self.results
    
    def save_results(self):
        """Save test results to file."""
        results_path = os.path.join(self.output_dir, 'results.json')
        with open(results_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        logger.info(f"Saved test results to {results_path}")
    
    def analyze_results(self) -> Dict:
        """Analyze test results.
        
        Returns:
            Dictionary with analysis results
        """
        if not self.results:
            logger.warning("No results to analyze")
            return {}
        
        # Create DataFrame
        df = pd.DataFrame(self.results)
        
        # Filter to successful generations
        success_df = df[df['success']]
        
        if success_df.empty:
            logger.warning("No successful generations to analyze")
            return {}
        
        # Extract scores
        for score_type in ['accuracy', 'relevance', 'engagement', 'mission_alignment', 'source_quality', 'average']:
            success_df[f'{score_type}_score'] = success_df['scores'].apply(lambda x: x.get(score_type, 0) if isinstance(x, dict) else 0)
        
        # Group by model and calculate metrics
        model_metrics = success_df.groupby('model').agg({
            'average_score': ['mean', 'std', 'count'],
            'accuracy_score': 'mean',
            'relevance_score': 'mean',
            'engagement_score': 'mean',
            'mission_alignment_score': 'mean',
            'source_quality_score': 'mean',
            'generation_time': ['mean', 'std'],
            'word_count': ['mean', 'std'],
            'source_count': ['mean', 'std']
        })
        
        # Flatten column names
        model_metrics.columns = ['_'.join(col).strip() for col in model_metrics.columns.values]
        model_metrics = model_metrics.reset_index()
        
        # Group by temperature and calculate metrics
        temp_metrics = success_df.groupby('temperature').agg({
            'average_score': ['mean', 'std', 'count'],
            'generation_time': ['mean', 'std'],
            'word_count': ['mean', 'std'],
            'source_count': ['mean', 'std']
        })
        
        # Flatten column names
        temp_metrics.columns = ['_'.join(col).strip() for col in temp_metrics.columns.values]
        temp_metrics = temp_metrics.reset_index()
        
        # Calculate success rate by model
        success_rate = df.groupby('model')['success'].mean().reset_index()
        success_rate.columns = ['model', 'success_rate']
        
        # Merge model metrics with success rate
        model_metrics = pd.merge(model_metrics, success_rate, on='model')
        
        # Create plots
        self._create_model_comparison_plot(success_df)
        self._create_score_breakdown_plot(success_df)
        self._create_temperature_plot(success_df)
        
        return {
            'model_metrics': model_metrics.to_dict(orient='records'),
            'temperature_metrics': temp_metrics.to_dict(orient='records'),
            'success_rate': success_rate.to_dict(orient='records'),
            'total_variants': len(df),
            'successful_variants': len(success_df),
            'content_items': len(df['content_id'].unique()),
            'models': list(df['model'].unique()),
            'temperatures': list(df['temperature'].unique())
        }
    
    def _create_model_comparison_plot(self, df: pd.DataFrame):
        """Create model comparison plot.
        
        Args:
            df: DataFrame with results
        """
        plt.figure(figsize=(10, 6))
        
        # Create grouped bar chart
        sns.barplot(x='model', y='average_score', data=df, ci='sd')
        
        plt.title('Model Comparison - Average Quality Score')
        plt.xlabel('Model')
        plt.ylabel('Average Quality Score')
        plt.ylim(0, 5)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        
        # Save plot
        plot_path = os.path.join(self.output_dir, 'model_comparison.png')
        plt.savefig(plot_path)
        plt.close()
    
    def _create_score_breakdown_plot(self, df: pd.DataFrame):
        """Create score breakdown plot.
        
        Args:
            df: DataFrame with results
        """
        # Melt DataFrame for plotting
        score_cols = ['accuracy_score', 'relevance_score', 'engagement_score', 
                     'mission_alignment_score', 'source_quality_score']
        
        melted = pd.melt(
            df,
            id_vars=['model', 'content_id'],
            value_vars=score_cols,
            var_name='score_type',
            value_name='score'
        )
        
        # Clean up score type names
        melted['score_type'] = melted['score_type'].str.replace('_score', '').str.replace('_', ' ').str.title()
        
        plt.figure(figsize=(12, 6))
        
        # Create grouped bar chart
        sns.barplot(x='score_type', y='score', hue='model', data=melted)
        
        plt.title('Score Breakdown by Model')
        plt.xlabel('Score Type')
        plt.ylabel('Average Score')
        plt.ylim(0, 5)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend(title='Model')
        plt.tight_layout()
        
        # Save plot
        plot_path = os.path.join(self.output_dir, 'score_breakdown.png')
        plt.savefig(plot_path)
        plt.close()
    
    def _create_temperature_plot(self, df: pd.DataFrame):
        """Create temperature comparison plot.
        
        Args:
            df: DataFrame with results
        """
        # Only create plot if multiple temperatures
        if len(df['temperature'].unique()) <= 1:
            return
        
        plt.figure(figsize=(10, 6))
        
        # Create grouped bar chart
        sns.barplot(x='temperature', y='average_score', hue='model', data=df)
        
        plt.title('Temperature Comparison - Average Quality Score')
        plt.xlabel('Temperature')
        plt.ylabel('Average Quality Score')
        plt.ylim(0, 5)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.legend(title='Model')
        plt.tight_layout()
        
        # Save plot
        plot_path = os.path.join(self.output_dir, 'temperature_comparison.png')
        plt.savefig(plot_path)
        plt.close()
    
    def generate_report(self) -> str:
        """Generate HTML report of test results.
        
        Returns:
            HTML report
        """
        # Analyze results
        analysis = self.analyze_results()
        
        if not analysis:
            return "<h1>No results to report</h1>"
        
        # Create HTML report
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A/B Test Results</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2, h3 {{ color: #333; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .metric-card {{ background-color: #f0f8ff; border-radius: 5px; padding: 15px; margin-bottom: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .metric-value {{ font-size: 24px; font-weight: bold; margin: 10px 0; }}
        .metric-label {{ font-size: 14px; color: #666; }}
        .container {{ display: flex; flex-wrap: wrap; gap: 15px; margin-bottom: 20px; }}
        .card {{ flex: 1; min-width: 200px; }}
        img {{ max-width: 100%; height: auto; border: 1px solid #ddd; margin-top: 10px; }}
    </style>
</head>
<body>
    <h1>A/B Test Results</h1>
    <p>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    
    <div class="container">
        <div class="card metric-card">
            <div class="metric-label">Content Items</div>
            <div class="metric-value">{analysis['content_items']}</div>
        </div>
        <div class="card metric-card">
            <div class="metric-label">Total Variants</div>
            <div class="metric-value">{analysis['total_variants']}</div>
        </div>
        <div class="card metric-card">
            <div class="metric-label">Successful Variants</div>
            <div class="metric-value">{analysis['successful_variants']}</div>
        </div>
        <div class="card metric-card">
            <div class="metric-label">Success Rate</div>
            <div class="metric-value">{analysis['successful_variants'] / analysis['total_variants']:.1%}</div>
        </div>
    </div>
    
    <h2>Model Metrics</h2>
    <table>
        <tr>
            <th>Model</th>
            <th>Avg. Score</th>
            <th>Accuracy</th>
            <th>Relevance</th>
            <th>Engagement</th>
            <th>Mission</th>
            <th>Sources</th>
            <th>Gen. Time</th>
            <th>Success Rate</th>
            <th>Count</th>
        </tr>
"""
        
        # Add model metrics rows
        for metric in analysis['model_metrics']:
            html += f"""
        <tr>
            <td>{metric['model']}</td>
            <td>{metric['average_score_mean']:.2f} ± {metric['average_score_std']:.2f}</td>
            <td>{metric['accuracy_score_mean']:.2f}</td>
            <td>{metric['relevance_score_mean']:.2f}</td>
            <td>{metric['engagement_score_mean']:.2f}</td>
            <td>{metric['mission_alignment_score_mean']:.2f}</td>
            <td>{metric['source_quality_score_mean']:.2f}</td>
            <td>{metric['generation_time_mean']:.1f}s</td>
            <td>{metric['success_rate']:.1%}</td>
            <td>{int(metric['average_score_count'])}</td>
        </tr>"""
        
        html += """
    </table>
    
    <h2>Temperature Metrics</h2>
    <table>
        <tr>
            <th>Temperature</th>
            <th>Avg. Score</th>
            <th>Gen. Time</th>
            <th>Word Count</th>
            <th>Source Count</th>
            <th>Count</th>
        </tr>
"""
        
        # Add temperature metrics rows
        for metric in analysis['temperature_metrics']:
            html += f"""
        <tr>
            <td>{metric['temperature']:.1f}</td>
            <td>{metric['average_score_mean']:.2f} ± {metric['average_score_std']:.2f}</td>
            <td>{metric['generation_time_mean']:.1f}s</td>
            <td>{metric['word_count_mean']:.0f} ± {metric['word_count_std']:.0f}</td>
            <td>{metric['source_count_mean']:.1f} ± {metric['source_count_std']:.1f}</td>
            <td>{int(metric['average_score_count'])}</td>
        </tr>"""
        
        html += """
    </table>
    
    <h2>Visualizations</h2>
    <div class="container">
        <div class="card">
            <h3>Model Comparison</h3>
            <img src="model_comparison.png" alt="Model Comparison">
        </div>
        <div class="card">
            <h3>Score Breakdown</h3>
            <img src="score_breakdown.png" alt="Score Breakdown">
        </div>
    </div>
"""
        
        # Add temperature comparison if multiple temperatures
        if len(analysis['temperatures']) > 1:
            html += """
    <div class="container">
        <div class="card">
            <h3>Temperature Comparison</h3>
            <img src="temperature_comparison.png" alt="Temperature Comparison">
        </div>
    </div>
"""
        
        html += """
    <h2>Individual Results</h2>
    <table>
        <tr>
            <th>Content ID</th>
            <th>Title</th>
            <th>Model</th>
            <th>Temp</th>
            <th>Success</th>
            <th>Quality</th>
            <th>Avg. Score</th>
            <th>Gen. Time</th>
            <th>Word Count</th>
            <th>Source Count</th>
        </tr>
"""
        
        # Add individual result rows
        for result in self.results:
            if result['success']:
                html += f"""
        <tr>
            <td>{result['content_id']}</td>
            <td>{result['title']}</td>
            <td>{result['model']}</td>
            <td>{result['temperature']:.1f}</td>
            <td>{"✓" if result['success'] else "✗"}</td>
            <td>{result.get('quality_rating', 'N/A')}</td>
            <td>{result['scores'].get('average', 0):.2f}</td>
            <td>{result['generation_time']:.1f}s</td>
            <td>{result.get('word_count', 0)}</td>
            <td>{result.get('source_count', 0)}</td>
        </tr>"""
            else:
                html += f"""
        <tr>
            <td>{result['content_id']}</td>
            <td>{result['title']}</td>
            <td>{result['model']}</td>
            <td>{result['temperature']:.1f}</td>
            <td>{"✓" if result['success'] else "✗"}</td>
            <td colspan="5">{result.get('error', 'Generation failed')}</td>
        </tr>"""
        
        html += """
    </table>
</body>
</html>
"""
        
        # Save report
        report_path = os.path.join(self.output_dir, 'report.html')
        with open(report_path, 'w') as f:
            f.write(html)
        
        logger.info(f"Saved report to {report_path}")
        
        return html


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="A/B Testing for Content Generation")
    
    # Content selection
    parser.add_argument("--content-ids", required=True, help="Comma-separated list of content IDs to test")
    
    # Model selection
    parser.add_argument("--models", required=True, help="Comma-separated list of models to test")
    
    # Temperature selection
    parser.add_argument("--temperatures", help="Comma-separated list of temperatures to test")
    
    # Output options
    parser.add_argument("--output-dir", default="ab_test_results", help="Directory to save results")
    
    # Generation options
    parser.add_argument("--force", action="store_true", help="Force generation even if dependencies aren't met")
    parser.add_argument("--delay", type=int, default=0, help="Delay between generations in seconds")
    
    args = parser.parse_args()
    
    # Parse content IDs
    content_ids = [cid.strip() for cid in args.content_ids.split(',')]
    
    # Parse models
    models = [model.strip() for model in args.models.split(',')]
    
    # Parse temperatures
    temperatures = None
    if args.temperatures:
        temperatures = [float(temp.strip()) for temp in args.temperatures.split(',')]
    
    # Create and run A/B test
    ab_test = ABTest(content_ids, models, temperatures, args.output_dir)
    ab_test.run_test(args.force, args.delay)
    
    # Generate report
    ab_test.generate_report()
    
    logger.info(f"A/B test complete. Results saved to {args.output_dir}")

if __name__ == "__main__":
    main()
